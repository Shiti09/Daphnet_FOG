{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Daphnet Freezing of Gait\n",
        "- 1 BUSINESS TASK- The goal of this project is to analyze the data(accelerometer data collected from 10 subjects suffering from Parkinson's Disease) and to recognize Freezing of Gait automatically.\n",
        "\n",
        "- 2 DATA COLLECTION AND UNDERSTANDING Data has been collected from UCI Machine Learning Repository --https://archive.ics.uci.edu/dataset/245/daphnet+freezing+of+gait The dataset includes 237 instances collected from 10 patients over 3 rounds(Users performed there kinds of tasks: R01- straight line walking, walking with numerous turns, R02- and finally a more realistic activity of daily living (ADL) task, R03- where users went into different rooms while fetching coffee, opening doors, etc.)\n",
        "\n",
        "- 3 DATA EXPLORATION In this step, we will apply Exploratory Data Analysis (EDA) to extract insights from the data set to know which features could help us in classifying a Freezing of Gait or otherwise a normal gait. Performing  Data Analysis using Pandas and Data visualization. Below are tasks to be performed in EDA:\n",
        "\n",
        "**Importing Libraries**\n",
        "\n",
        "**Data Cleaning and EDA**\n"
      ],
      "metadata": {
        "id": "cMDcpclrmjE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading libraries**"
      ],
      "metadata": {
        "id": "8d7SPRamYRH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "YMUZIV4b2Yyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Clean Data**"
      ],
      "metadata": {
        "id": "4mEJD3GRZC4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [\n",
        "    \"S01R01.txt\", \"S01R02.txt\", \"S02R01.txt\", \"S02R02.txt\", \"S03R01.txt\", \"S03R02.txt\",\n",
        "    \"S03R03.txt\", \"S04R01.txt\", \"S05R01.txt\", \"S05R02.txt\", \"S06R01.txt\", \"S06R02.txt\",\n",
        "    \"S07R01.txt\", \"S07R02.txt\", \"S08R01.txt\", \"S09R01.txt\", \"S10R01.txt\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "for i, file in enumerate(file_paths):\n",
        "    df = pd.read_csv(file, header=None, delimiter=' ', on_bad_lines='skip')\n",
        "    df['Subject'] = f\"S{i+1}\"\n",
        "    dfs.append(df)\n",
        "\n",
        "df_combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_combined.columns = ['Time_stamp', 'Ankle_x', 'Ankle_y', 'Ankle_z', 'Thigh_x', 'Thigh_y',\n",
        "                       'Thigh_z', 'Trunk_x', 'Trunk_y', 'Trunk_z', 'Annotations', 'Subject']\n",
        "\n",
        "df_combined.dropna(inplace=True)\n",
        "df_combined = df_combined[df_combined['Annotations'] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRkvFbVECm3I",
        "outputId": "9c79e7ad-85a1-4c09-8338-265da105fd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0   1   2    3  4  5  6  7  8  9  10 Subject\n",
            "0  15  70  39 -970  0  0  0  0  0  0   0     S01\n",
            "1  31  70  39 -970  0  0  0  0  0  0   0     S01\n",
            "2  46  60  49 -960  0  0  0  0  0  0   0     S01\n",
            "3  62  60  49 -960  0  0  0  0  0  0   0     S01\n",
            "4  78  50  39 -960  0  0  0  0  0  0   0     S01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check class distribution**"
      ],
      "metadata": {
        "id": "dkq3MINWekIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcMlRXQMYuSL"
      },
      "outputs": [],
      "source": [
        "print(df_combined['Annotations'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot class distribution**"
      ],
      "metadata": {
        "id": "qQFfSzhmfAv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df_combined['Annotations'].value_counts().plot(kind='bar')\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2SSAH8u3ksXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finding missing values**"
      ],
      "metadata": {
        "id": "1S_CjiPDfTcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.isnull().sum()"
      ],
      "metadata": {
        "id": "uMYO1xZZbjlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df_combined[df_combined.isnull().any(axis=1)]\n",
        "missing_values"
      ],
      "metadata": {
        "id": "b-6qixL6-H45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Label Encoding data**"
      ],
      "metadata": {
        "id": "Ps-YtzU7gFBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features & Labels\n",
        "X = df_combined.drop(columns=['Annotations', 'Subject'])\n",
        "y = df_combined['Annotations']\n",
        "\n",
        "# Encode Labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "7S7dZxzadKro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting data**"
      ],
      "metadata": {
        "id": "cnIftJZMgPXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "denluTqnx5FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**"
      ],
      "metadata": {
        "id": "5mMXosSZgY11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation (Jittering)\n",
        "def augment_data(data, factor=5):\n",
        "    jittered_data = np.tile(data, (factor, 1))  # Duplicate data\n",
        "    noise = np.random.normal(0, 0.01, jittered_data.shape)  # Add slight noise\n",
        "    return jittered_data + noise"
      ],
      "metadata": {
        "id": "XNtSqikfgqVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Balancing Dataset**"
      ],
      "metadata": {
        "id": "5c_Y9naVg0RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance Dataset\n",
        "freeze_data = X_train[y_train == 1]\n",
        "no_freeze_data = X_train[y_train == 0]\n",
        "\n",
        "augmented_freeze = augment_data(freeze_data, factor=(len(no_freeze_data) // len(freeze_data)))\n",
        "X_train_balanced = np.vstack([no_freeze_data, augmented_freeze])\n",
        "y_train_balanced = np.hstack([np.zeros(len(no_freeze_data)), np.ones(len(augmented_freeze))])\n",
        "\n",
        "print(\"After augmentation:\", dict(Counter(y_train_balanced)))"
      ],
      "metadata": {
        "id": "UymL1af3gzea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalizing Data**"
      ],
      "metadata": {
        "id": "oRHFxMWGhAne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "unIoOlWIgzph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reshaping for LSTM**"
      ],
      "metadata": {
        "id": "U6NWkmoOhJP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_balanced = X_train_balanced.reshape(X_train_balanced.shape[0], X_train_balanced.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "y_train_balanced = to_categorical(y_train_balanced, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)"
      ],
      "metadata": {
        "id": "O_xUxMkbgzte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**"
      ],
      "metadata": {
        "id": "7UAkux6bhR04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Bidirectional(LSTM(32, return_sequences=True, input_shape=(X_train_balanced.shape[1], 1))),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(32, return_sequences=False)),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "class_weights = {0: 1, 1: len(no_freeze_data) / len(augmented_freeze)}\n",
        "model.compile(optimizer=AdamW(learning_rate=1e-4), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "6WDiYgI4gzwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the model**"
      ],
      "metadata": {
        "id": "KLa5IHR-hcba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_balanced, y_train_balanced,\n",
        "    epochs=30, batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "jl1-w2DYhgZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating the model**"
      ],
      "metadata": {
        "id": "8JLLmirYhnIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}, Test Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "SZ7TS8C8hgnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Report**"
      ],
      "metadata": {
        "id": "BtEvKJVChwGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "7HX--XJWhvcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSHzP5aNhgqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSUHBFbrgluX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}